<!DOCTYPE html>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="robots" content="noindex, nofollow">
  <meta name="googlebot" content="noindex, nofollow">
  <script type="text/javascript" src="./jquery-3.1.0.js"></script>
    <link rel="stylesheet" type="text/css" href="./result-light.css">
    
      <script type="text/javascript" src="./bootstrap.min.js"></script>
    
      <link rel="stylesheet" type="text/css" href="./bootstrap-theme.min.css">
    
      <link rel="stylesheet" type="text/css" href="./bootstrap.min.css">
    
      <script type="text/javascript" src="./affdex.js"></script>
  <style type="text/css">
    
  </style>

  <title>Emotion from Camera Sample App</title>
  <style>
.center {
    margin: auto;
    width: 60%;
    border: 3px solid #73AD21;
    padding: 10px;
}
</style>
  
</head>

<body>
 
  <div id="affdex_elements" style="width:680px;height:480px;margin:auto;padding-top:50px;">
    </div>
    <img src="wm_logo.jpg"style="width:400px;padding-top:100px;">


  
<script type="text/javascript" src="eventemitter2.min.js"></script>
<script type="text/javascript" src="roslib.min.js"></script>



<script type="text/javascript">//<![CDATA[
      // Connecting to ROS
      // -----------------

      var ros = new ROSLIB.Ros({
        url : 'ws://localhost:9090'
      });

      ros.on('connection', function() {
        console.log('Connected to websocket server.');
      });

      ros.on('error', function(error) {
        console.log('Error connecting to websocket server: ', error);
      });

      ros.on('close', function() {
        console.log('Connection to websocket server closed.');
      });

      var control_emo = new ROSLIB.Topic({
            ros : ros,
            name : '/control_emo',
            messageType : 'std_msgs/UInt8'
        });
      var face_mode = new ROSLIB.Topic({
            ros : ros,
            name : '/face_mode',
            messageType : 'std_msgs/UInt8'
        });
    
      var smile = new ROSLIB.Message({
        data: 1
      });
      var circonspect = new ROSLIB.Message({
        data: 3
      });
      var rage = new ROSLIB.Message({
        data: 4
      });
      var sad = new ROSLIB.Message({
        data: 2
      });
      var wink = new ROSLIB.Message({
        data: 6
      });
      var scream = new ROSLIB.Message({
        data: 5
      });
      var mode = new ROSLIB.Message({
        data: 3
      });
    
      face_mode.publish(mode); 
    
      // SDK Needs to create video and canvas nodes in the DOM in order to function
      // Here we are adding those nodes a predefined div.
      var divRoot = $("#affdex_elements")[0];
      var width = 640;
      var height = 480;
      var faceMode = affdex.FaceDetectorMode.SMALL_FACES;
      //Construct a CameraDetector and specify the image width / height and face detector mode.
      var detector = new affdex.CameraDetector(divRoot, width, height, faceMode);
      var actualEmo = "smile";
      
      //setTimeout("location.reload(true);", 60000);
      
    
      //Enable detection of all Expressions, Emotions and Emojis classifiers.
      detector.detectAllEmotions();
      detector.detectAllExpressions();
      detector.detectAllEmojis();
      detector.detectAllAppearance();
      onStart() 
      //Add a callback to notify when the detector is initialized and ready for runing.
      detector.addEventListener("onInitializeSuccess", function() {
        log('#logs', "The detector reports initialized");
        //Display canvas instead of video feed because we want to draw the feature points on it
        $("#face_video_canvas").css("display", "block");
        $("#face_video").css("display", "none");
      });
        
    
      function sleep(milliseconds) {
          var start = new Date().getTime();
          for (var i = 0; i < 1e7; i++) {
            if ((new Date().getTime() - start) > milliseconds){
              break;
            }
          }
        }
    
      function log(node_name, msg) {
        $(node_name).append("<span>" + msg + "</span><br />")
      }

      //function executes when Start button is pushed.
      function onStart() {
        if (detector && !detector.isRunning) {
          $("#logs").html("");
          detector.start();
        }
        log('#logs', "Clicked the start button");
      }

      //function executes when the Stop button is pushed.
      function onStop() {
        log('#logs', "Clicked the stop button");
        if (detector && detector.isRunning) {
          detector.removeEventListener();
          detector.stop();
        }
      };

      //function executes when the Reset button is pushed.
      function onReset() {
        log('#logs', "Clicked the reset button");
        if (detector && detector.isRunning) {
          detector.reset();

          $('#results').html("");
        }
      };

      //Add a callback to notify when camera access is allowed
      detector.addEventListener("onWebcamConnectSuccess", function() {
        log('#logs', "Webcam access allowed");
      });

      //Add a callback to notify when camera access is denied
      detector.addEventListener("onWebcamConnectFailure", function() {
        log('#logs', "webcam denied");
        console.log("Webcam access denied");
      });

      //Add a callback to notify when detector is stopped
      detector.addEventListener("onStopSuccess", function() {
        log('#logs', "The detector reports stopped");
        $("#results").html("");
      });

      //Add a callback to receive the results from processing an image.
      //The faces object contains the list of the faces detected in an image.
      //Faces object contains probabilities for all the different expressions, emotions and appearance metrics
      detector.addEventListener("onImageResultsSuccess", function(faces, image, timestamp) {
        $('#results').html("");

        
          
        log('#results', "Timestamp: " + timestamp.toFixed(2));
        log('#results', "Number of faces found: " + faces.length);
        if (faces.length > 0) {

          var contxt = $('#face_video_canvas')[0].getContext('2d');
          contxt.strokeStyle = "#FFFFFF";
          var hRatio = contxt.canvas.width / image.width;
          var vRatio = contxt.canvas.height / image.height;
          var ratio = Math.min(hRatio, vRatio);  
            
            
          log('#results', "Age: " + JSON.stringify(faces[0].appearance.age));
          log('#results', "Gender: " + JSON.stringify(faces[0].appearance.gender));
          log('#results', "Ethnicity: " + JSON.stringify(faces[0].appearance.ethnicity));
          log('#results', "Lunettes: " + JSON.stringify(faces[0].appearance.glasses));
          log('#results', "Emotions: " + JSON.stringify(faces[0].emotions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Expressions: " + JSON.stringify(faces[0].expressions, function(key, val) {
            return val.toFixed ? Number(val.toFixed(0)) : val;
          }));
          log('#results', "Emoji: " + faces[0].emojis.dominantEmoji);
          var minX =9999;
          var minY =9999;
          var maxX =0;
          var maxY =0;
          for (var id in faces[0].featurePoints) {
            if(faces[0].featurePoints[id].x < minX)
                minX = faces[0].featurePoints[id].x;
            if(faces[0].featurePoints[id].x > maxX)
                maxX = faces[0].featurePoints[id].x;
            if(faces[0].featurePoints[id].y < minY)
                minY = faces[0].featurePoints[id].y;
            if(faces[0].featurePoints[id].y > maxY)
                maxY = faces[0].featurePoints[id].y;

          //drawFeaturePoints(image, faces[0].featurePoints);
          }
          contxt.beginPath();
          contxt.rect(minX,minY,maxX-minX,maxY-minY);
          contxt.stroke();
          contxt.beginPath();
          contxt.font="20px Arial";
          contxt.fillText("Age : "+faces[0].appearance.age, maxX+5, minY);
          contxt.fillText("Gender : "+faces[0].appearance.gender, maxX+5, minY+25);
          contxt.fillText("Glasses : "+faces[0].appearance.glasses, maxX+5, minY+50);
          contxt.fillText("Ethnicity : "+faces[0].appearance.ethnicity, maxX+5, minY+75);
          if (faces[0].emojis.smiley > 50 && actualEmo != "smile"){
              control_emo.publish(smile); 
              actualEmo = "smile";
          }else if (faces[0].emojis.flushed > 70 && actualEmo != "circonspect"){
              control_emo.publish(circonspect); 
              actualEmo = "circonspect";
          }else if (faces[0].emojis.rage > 70 && actualEmo != "rage"){
              control_emo.publish(rage); 
              actualEmo = "rage"; 
          }else if (faces[0].emojis.disappointed > 70 && actualEmo != "disappointed"){
              control_emo.publish(sad); 
              actualEmo = "disappointed"; 
          }else if (faces[0].emojis.wink > 70 && actualEmo != "wink"){
              control_emo.publish(wink); 
              actualEmo = "wink"; 
          }else if (faces[0].emojis.scream > 70 && actualEmo != "scream"){
              control_emo.publish(scream); 
              actualEmo = "scream"; 
          }
          contxt.fillText("Smile : " +  Math.round(faces[0].emojis.smiley,2), minX-100, minY);
          contxt.fillText("Flushed : " + Math.round(faces[0].emojis.flushed,2), minX-100, minY+25);
          contxt.fillText("Rage : " + Math.round(faces[0].emojis.rage,2), minX-100, minY+50);
          contxt.fillText("Sad : " + Math.round(faces[0].emojis.disappointed,2), minX-100, minY+75);
          contxt.fillText("Wink : " + Math.round(faces[0].emojis.wink,2), minX-100, minY+100);
          contxt.fillText("Scream : " + Math.round(faces[0].emojis.scream,2), minX-100, minY+125);
            
          contxt.strokeText(faces[0].emojis.dominantEmoji, maxX+5, minY+100);
          contxt.font="100px Arial";
           
          contxt.stroke();
        }
      });
      

    
      //Draw the detected facial feature points on the image
      function drawFeaturePoints(img, featurePoints) {
        var contxt = $('#face_video_canvas')[0].getContext('2d');

        var hRatio = contxt.canvas.width / img.width;
        var vRatio = contxt.canvas.height / img.height;
        var ratio = Math.min(hRatio, vRatio);

        contxt.strokeStyle = "#FFFFFF";
        for (var id in featurePoints) {
          contxt.beginPath();
          contxt.arc(featurePoints[id].x,
            featurePoints[id].y, 2, 0, 2 * Math.PI);
          contxt.stroke();

        }
      }



</script>

  <script>
  // tell the embed parent frame the height of the content
  if (window.parent && window.parent.parent){
    window.parent.parent.postMessage(["resultsFrame", {
      height: document.body.getBoundingClientRect().height,
      slug: "ajz9upk2"
    }], "*")
  }
</script>





</body></html>